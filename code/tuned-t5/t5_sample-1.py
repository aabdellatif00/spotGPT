# -*- coding: utf-8 -*-
"""T5_Sample.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CyNIOUNHGNJgYZP9T9HnL2u-cZG1mtvy
"""

#!pip install transformers
#!pip install sentencepiece
#!pip install torch

import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch
from scoring import get_log_probability, get_likelihood_ratio
from sklearn.model_selection import train_test_split
import numpy 
import seaborn
import matplotlib.pyplot as plt
data = pd.read_csv("drive/MyDrive/GPT-wiki-intro.csv")

# create a DataFrame from the list of dictionaries
df = data[['id', 'wiki_intro', 'generated_intro']].sample(n=1500, random_state=42)
df = df.rename(columns={'wiki_intro': 'source_text', 'generated_intro': 'target_text'})

#data.head()

# Add prefix to source text
df['source_text'] = "summarize: " + df['source_text']

# Split the dataset into train and test sets
train_df, test_df = train_test_split(df, test_size=0.15)

# Instantiate the tokenizer and model
tokenizer = T5Tokenizer.from_pretrained('t5-small')
model = T5ForConditionalGeneration.from_pretrained('t5-small')

# Tokenize the source and target texts
train_encodings = tokenizer(list(train_df['source_text']), truncation=True, padding=True)
train_labels = tokenizer(list(train_df['target_text']), truncation=True, padding=True)

# Fine-tune the model on the train set
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)
for epoch in range(1):
    model.train()
    train_loss = 0
    for i in range(len(train_df)):
        input_ids = torch.tensor(train_encodings['input_ids'][i]).unsqueeze(0)
        attention_mask = torch.tensor(train_encodings['attention_mask'][i]).unsqueeze(0)
        labels = torch.tensor(train_labels['input_ids'][i]).unsqueeze(0)
        output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = output.loss
        train_loss += loss.item()
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        print(f"Step {i} of {len(train_df)}")
    print(f"Epoch {epoch + 1} Train Loss: {train_loss / len(train_df)}")

# Save the model to a file
torch.save(model.state_dict(), 'model.pth')

# Create a tar file and add the model file to it
with tarfile.open('model.tar', 'w') as tar:
    tar.add('model.pth')

# Initialize an empty list to store the generated summaries
human_predictions = []

# Loop over each item in data['text']
for text in test_df['source_text']:
    # Encode the text using the tokenizer
    input_ids = tokenizer.encode(text, return_tensors='pt')
    
    # Generate a summary using the model
    summary_ids = model.generate(input_ids, max_length=100, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    
    # Add the generated summary to the list
    human_predictions.append(summary)

# Initialize an empty list to store the generated summaries
machine_predictions = []

# Loop over each item in data['text']
for text in test_df['target_text']:
    # Encode the text using the tokenizer
    input_ids = tokenizer.encode(text, return_tensors='pt')
    
    # Generate a summary using the model
    summary_ids = model.generate(input_ids, max_length=100, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    
    # Add the generated summary to the list
    machine_predictions.append(summary)

machine_predictions

t5_df = test_df.copy()
t5_df['human_reconstructed'] = human_predictions
t5_df['machine_reconstructed'] = machine_predictions
t5_df = t5_df.rename(columns={'source_text': 'human_original', 'target_text': 'machine_original'})
t5_df.to_csv('t5_df_final.csv', index=False)

t5_data = pd.read_csv("sample_data/t5_df.csv")

for idx, row in t5_data.iterrows():
  row["human_original"] = row["human_original"][11:]
  row["machine_original"] = row["machine_original"][11:]
  

plot_dataframe = pd.DataFrame()
for idx, row in t5_data.iterrows():
    hgt_to_add = {'Type' : 'HGT', 'text' : 
                  row["human_original"][11:], 
                  'text_reconstructed' : row["human_reconstructed"]}
    mgt_to_add = {'Type' : 'MGT', 
                  'text' : row["machine_original"][11:], 
                  'text_reconstructed' : row["machine_reconstructed"]}
    plot_dataframe = plot_dataframe.append(mgt_to_add, ignore_index = True)
    plot_dataframe = plot_dataframe.append(hgt_to_add, ignore_index = True)

probability_ratios = [0] * len(plot_dataframe)
for idx, row in plot_dataframe.iterrows():
    text = row['text']
    sampled_text = row['text_reconstructed']
    real_lp = get_log_probability(text)
    sampled_lp = get_log_probability(sampled_text)
    print(real_lp, sampled_lp)
    probability_ratio = get_likelihood_ratio(real_lp, sampled_lp)
    probability_ratios[idx] = probability_ratio
plot_dataframe['probability_ratio'] = probability_ratios


MGT = plot_dataframe[plot_dataframe['Type'] == "MGT"]["probability_ratio"]
HGT = plot_dataframe[plot_dataframe['Type'] == "HGT"]["probability_ratio"]

fig, ax = plt.subplots()

sns.kdeplot(data = plot_dataframe, 
                x = "probability_ratio", 
                hue = "Type",
                fill=True, 
                common_norm=False, 
                alpha=.5, 
                linewidth=0,
                ax = ax
).set(title='Density of Probability Ratio Scores (T5)', 
      xlabel='Probability Ratio Score', 
      ylabel='Density')
ax.set_xlim(0, 20000)
plt.savefig("density_estimate_t5.png")

